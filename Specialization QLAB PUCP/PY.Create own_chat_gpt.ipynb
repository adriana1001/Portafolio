{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27rrkfxmTR7x"
   },
   "source": [
    "# Chat With Your Data\n",
    "\n",
    "In this course, we will endeavor to replicate [chatpdf](https://www.chatpdf.com/) and the essence of all natural language models that operate by responding within a specific context (a document).\n",
    "\n",
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqu9Y0toTR7y",
    "outputId": "e776ed60-b384-4647-b680-f9a0079215c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai pypdf python-dotenv chromadb  tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi7eIc3fpjOr"
   },
   "source": [
    "![](https://www.dropbox.com/scl/fi/cevg8k5kcjav1meihfr8m/embedding_1.png?rlkey=vsp2v4kk7k6r2xzv1dcqjbonh&dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUaCRzJ9px-2"
   },
   "source": [
    "![](https://www.dropbox.com/scl/fi/z3wlqq3q5sa3njzkhuwrs/embedding_2.png?rlkey=mqp453qxzvpsm5ne6ymrcai3w&dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUAgKWvqEgi"
   },
   "source": [
    "![](https://www.dropbox.com/scl/fi/13g4kkcgmzhoj72ift91u/embedding_3.png?rlkey=0l6ieif0gcr92tkz6c3sdsdkf&dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_5u0K5nTR7y"
   },
   "source": [
    "## Structure\n",
    "\n",
    "- API keys and Environment Variables\n",
    "- Document Loading\n",
    "- Document Splitting\n",
    "- Vectorstores and Embedding (Storage)\n",
    "- Retrieval\n",
    "- Question Answering\n",
    "- Chat\n",
    "\n",
    "\n",
    "<!-- ![](figs/0_preview.png) -->\n",
    "![](https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
    "\n",
    "In this tutorial, we will delve into the essential steps for creating a natural language model. We'll begin by understanding the significance of API keys and utilizing environment variables to ensure security and privacy in our applications. Next, we'll dive into document loading, mastering the handling of various file types and data sources. Following that, we'll tackle document splitting for efficient processing. We'll then proceed to create vector stores and embeddings, crucial for representing the semantic meaning of words. Afterward, we'll explore information retrieval techniques and question-answering capabilities, culminating in the implementation of a chat system based on our model.\n",
    "\n",
    "\n",
    "\n",
    "## API Key and Environment Variables\n",
    "\n",
    "Creating the API key from OPENAI, [here](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "Once you have obtained the API key, it needs to be securely stored.\n",
    "\n",
    "There are various methods for loading this API key. One approach is to utilize environment files, which should ideally be private and included in the `.gitignore` if working in a collaborative environment. Github, for instance, detects if any keys are uploaded to the platform, triggering an alert and disabling the API key, necessitating the creation of a new one. The key can be manually entered as well.\n",
    "\n",
    "### `Dotenv`\n",
    "\n",
    "For this method, you can create a `.env` file in the working environment and list the variables as follows: `variable = \"value_variable\"`.\n",
    "\n",
    "```plaintext\n",
    "NAME_OF_VARIABLE=\"sk-xxxxxxxxxxxxxxxx\"\n",
    "```\n",
    "\n",
    "To use it, `python-dotenv` is employed, which, through the functions `load_dotenv` and `find_dotenv`, loads the variables from the `.env` file.\n",
    "\n",
    "```python\n",
    "# `!pip install python-dotenv`\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "secret_variable = os.environ['NAME_OF_VARIABLE']\n",
    "```\n",
    "\n",
    "### Colab\n",
    "\n",
    "For Colab, a form with `getpass` can be introduced, which conceals the API key when entered. However, the drawback compared to the previous method is that the API key needs to be pasted each time the file is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwRBdvtCTR7z",
    "outputId": "e9165ce9-ab2b-4606-c4dd-6bbfb14b23cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI - KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai\n",
    "import getpass, openai, os\n",
    "api_key = getpass.getpass(prompt=\"OPENAI - KEY: \")\n",
    "openai.apikey = api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJFvpiw-TR7z"
   },
   "source": [
    "## Document Loading\n",
    "\n",
    "When it comes to loading documents, two scenarios must be considered: whether the information will be extracted from the web or if it's within our working environment. In the former case, it's possible (although `langchain` already has these cases implemented) that we'll need the `requests` library to download the file or its content. For the latter case, only the relative or absolute path of the file is sufficient.\n",
    "\n",
    "All documents follow this structure, returning a list of `Document` objects containing two sub-objects: `page_content`, which is the text within, and `metadata`.\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import `Method`\n",
    "file = Method(file_path)\n",
    "file_read = file.lod()\n",
    "print(file_read[0])\n",
    "Document(\n",
    "    page_content: \"text\",\n",
    "    metadata: {\"source\": file_path, ...}\n",
    ")\n",
    "```\n",
    "\n",
    "### PDFs\n",
    "\n",
    "For PDF files, it's already implemented to read the document by URL and local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVNV5yIYTR7z",
    "outputId": "81a5e8a6-208d-42d4-e35a-7a406f876657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Z. Shen et al.\n",
      "Table 2: All operations supported by the layout elements. The same APIs are\n",
      "supported across diﬀerent layout element classes including Coordinate types,\n",
      "TextBlock andLayout .\n",
      "Operation Name Description\n",
      "block.pad(top, bottom, right, left) Enlarge the current block according to the input\n",
      "block.scale(fx, fy)Scale the current block given the ratio\n",
      "in x and y direction\n",
      "block.shift(dx, dy)Move the current block with the shift\n",
      "distances in x and y direction\n",
      "block1.is in(block2) Whether\n",
      "{'source': 'https://arxiv.org/pdf/2103.15348.pdf', 'page': 7}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(pages[7].page_content[:500])\n",
    "print(pages[7].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwBfls21TR7z"
   },
   "source": [
    "### Web Plain Text\n",
    "\n",
    "For plain text from a URL, the `WebBaseLoader` can be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMbVTehGTR7z",
    "outputId": "2e9f5202-974f-446a-a10a-0d27825092cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Getting Started\\n\\nGetting started at 37signals involves a lot of little details, a number of big tasks, learning the details of your new job, meeting new coworkers, all while working remotely. Your teammates, your manager, your 37signals buddy, your Ops buddy, and our People team are all here to help as you navigate your first few days and weeks.\\n\\n## Your First Few Days\\n\\nBefore you start, the People team will order you a new Apple laptop with the specs you request and any accessories you need like an external keyboard, mouse, or display. Get what you need, while keeping in mind the demands of your work when choosing specs.\\n\\nA day or two before you start, your manager will email you instructions for your first day. Your manager will be your point of contact for your early projects and activities. You‚Äôll also work with a member of our Ops team who will help you as you set up all the accounts you need to work at 37signals.\\n\\nOn your first day, you‚Äôll log into Basecamp to see a project dedicated to your onboarding called ‚ÄúWelcome, [your name]!‚Äù. Your welcome project will contain a few to-do lists, tailored to your role and linking to accounts or services that you need to set up. You‚Äôll also see to-do lists that your Ops buddy and manager will be working through. Your Ops buddy and your manager will be in contact with you as you set up your environment, should you have questions or get stuck.\\n\\nTo keep everyone's devices safe and secure, we manage all our Mac devices using [Kandji](https://kandji.io) as well as our in-house tool [Shipshape \\uf8ffüîí](https://github.com/basecamp/shipshape/).\\n\\nYour training schedules and [onboarding expectations](https://github.com/basecamp/handbook/blob/master/making-a-career.md#your-first-90-days) will be in your welcome project. You‚Äôll also find docs with helpful links to technical documentation, walkthrough videos, important Basecamp projects, and more.\\n\\nYour welcome project will also contain a list of people to meet over the course of your first few weeks.\\n\\n* Your manager. You and your manager will meet on your first day to sort out a recurring 1:1 schedule and to review your specific onboarding expectations.\\n* Your team. Most teams have a weekly call which you‚Äôll join your first week.\\n* Your 37signals buddy. Your buddy will be someone who‚Äôs not on your immediate team, who‚Äôs worked at 37signals for a while, and who can point you in the right direction when you have a question and don‚Äôt quite know where to turn. They‚Äôll introduce themselves during your first week, to say hi and offer their help. They‚Äôll check in with you periodically throughout your first couple months to see how you‚Äôre settling in.\\n* People Ops. You‚Äôll meet with Andrea to review 37signals policies, benefits, and compensation.\\n\\nAfter your environment has been set up, expectations are clear, and you‚Äôve met who you need to meet, you‚Äôll get to work! You‚Äôll begin working on real projects, with plenty of support and guidance, within your first week. Your responsibilities and impact will increase as you learn and gain confidence over the next few months, and most people are up to speed and feeling fully part of the team in about 3 months.\\n\", metadata={'source': 'https://raw.githubusercontent.com/basecamp/handbook/master/getting-started.md'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/getting-started.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZAnHajmTR7z"
   },
   "source": [
    "### JSON\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "loader = JSONLoader(\n",
    "    file_path=\"\",\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "```\n",
    "\n",
    "For other documents, the documentation of [Langchain - Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be referred to.\n",
    "\n",
    "## Document Splitting\n",
    "\n",
    "Splitting the text of a document in LLM (Deep Learning Language Models) can be advantageous for several reasons. Firstly, it helps manage long documents, as LLMs may struggle with processing very large texts due to memory constraints or computational limitations. It improves contextual representation by capturing local contextual relationships more effectively.\n",
    "\n",
    "All methods of `langchain.text_splitter` have the following parameters\n",
    "\n",
    "- `separator=\"\\n\"`: Character used as a separator between parts of the text (e.g., line breaks).\n",
    "- `chunk_size=100`: Maximum size of each text fragment.\n",
    "- `chunk_overlap=20`: Overlap of characters between consecutive fragments.\n",
    "- `length_function`: A function that may dynamically adjust the fragment size, though its specific function is unclear without further context.\n",
    "\n",
    "\n",
    "\n",
    "### Split by character\n",
    "\n",
    "Splits text based on a user defined character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crZGQ38YTR7z",
    "outputId": "b5bb4455-be20-4a00-a109-b9fa9edb78a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# How We Work\n",
      "\n",
      "## Remotely\n",
      "\n",
      "37signals is a fully distributed company. Our team works from all over the world, across 5 continents. We don't care where employees choose to live and work, just that they're here to do great work on exceptional products, alongside a world-class team. We’ve been remote since we started, and our founders literally [wrote the book](https://basecamp.com/books/remote) on the subject.\n",
      "\n",
      "You can work from anywhere, but please be sure to inform your People Ops team when you \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "markdown = WebBaseLoader(\"https://raw.githubusercontent.com/basecamp/handbook/master/how-we-work.md\")\n",
    "markdown_doc = markdown.load()\n",
    "text_markdown = markdown_doc[0].page_content\n",
    "print(text_markdown[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHAOLnJrTR7z",
    "outputId": "32bd05ed-659c-4543-bc49-86005e146bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# How We Work\n",
      "\n",
      "## Remotely\n",
      "\n",
      "37signals is a fully distributed company. Our team works from all over the world, across 5 continents. We don't care where employees choose to live and work, just that they're here to do great work on exceptional products, alongside a world-class team. We’ve been remote since we started, and our founders literally [wrote the book](https://basecamp.com/books/remote) on the subject.\n",
      "\n",
      "You can work from anywhere, but please be sure to inform your People Ops team when you move – especially across state or country borders. It may affect your or the company’s tax situation.\n",
      "\n",
      "## Cycles\n",
      "\n",
      "We work in 6-week cycles at 37signals. This fixed cadence serves to give us an internal sense of urgency, to keep projects from ballooning, and to provide us with a regular interval to make decisions about what we’re working on.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_splitted = text_splitter.split_text(text_markdown)\n",
    "print(text_splitted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXwB-bAvTR70"
   },
   "source": [
    "### Split for markdown\n",
    "\n",
    "Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axQsog6sTR70",
    "outputId": "cf67fbcb-0fb1-4a0b-c23e-45bc8d5a534d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"37signals is a fully distributed company. Our team works from all over the world, across 5 continents. We don't care where employees choose to live and work, just that they're here to do great work on exceptional products, alongside a world-class team. We’ve been remote since we started, and our founders literally [wrote the book](https://basecamp.com/books/remote) on the subject.  \\nYou can work from anywhere, but please be sure to inform your People Ops team when you move – especially across state or country borders. It may affect your or the company’s tax situation.\", metadata={'Header 1': 'How We Work', 'Header 2': 'Remotely'}),\n",
       " Document(page_content='We work in 6-week cycles at 37signals. This fixed cadence serves to give us an internal sense of urgency, to keep projects from ballooning, and to provide us with a regular interval to make decisions about what we’re working on.  \\nOur cycle structure is particularly important for the product teams, since they approach feature and product development with scope and budget in mind up front. For more on this, all employees are encouraged to read [Shape Up](https://basecamp.com/shapeup/0.3-chapter-01#six-week-cycles).  \\nAll teams operate on the same 6-week cadence.', metadata={'Header 1': 'How We Work', 'Header 2': 'Cycles'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "md_header_splits = markdown_splitter.split_text(text_markdown)\n",
    "md_header_splits[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv6Iv957TR70"
   },
   "source": [
    "### Split for Code\n",
    "\n",
    "Splits text based on characters specific to coding languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1H1Toh_NTR70",
    "outputId": "904b0217-91ad-4157-d9f4-9ef2ca28c0ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import numpy as np'),\n",
       " Document(page_content='def rand():\\n    np.random.randint()'),\n",
       " Document(page_content='# Call the function\\nrand()')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "import numpy as np\n",
    "def rand():\n",
    "    np.random.randint()\n",
    "\n",
    "# Call the function\n",
    "rand()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_ekKSCfTR70",
    "outputId": "9fbf4cf0-96da-42cf-b972-964996cf131d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5LDBIcDTR70"
   },
   "source": [
    "## Embedding and Vectorstores (Storage)\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Embeddings are vector representations of words in a dimensional space, learned during training. They capture semantic and contextual meaning to facilitate the model's understanding and processing of the text.\n",
    "\n",
    "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMXbffSjTR70",
    "outputId": "15b47ea1-ef2a-49fe-98be-25ccf666a564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "embeddings = embedding.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Hello\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYsFsD3_TR70"
   },
   "source": [
    "This code snippet demonstrates how to use the `OpenAIEmbeddings` class from LangChain to embed multiple documents. It initializes an instance of the class, embeds the provided documents, and prints the length of the embeddings along with a sample of the first embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSorDcuZTR70",
    "outputId": "8f4efb0e-6ebb-48c6-9522-9421147201ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1536 \n",
      "vector_sample:  [-0.020291369630971504, -0.00707277412171542, -0.022869059830264393]\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings[0]\n",
    "print(\"length: \", len(text_embedding), \"\\nvector_sample: \" ,text_embedding[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1-7rF6WTR70"
   },
   "source": [
    "This code snippet further explores the embeddings obtained in the previous example. It prints the length of the first embedding vector and a sample of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZcLakyXTR70",
    "outputId": "2f28c715-872c-4a11-b2da-3027946af00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(pages)\n",
    "print(type(splits[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKPFFQI2TR71"
   },
   "source": [
    "Lastly, this code snippet demonstrates how to use the `RecursiveCharacterTextSplitter` class from LangChain to split documents into smaller chunks. It loads a PDF document from a URL, splits it into chunks, and stores the resulting chunks in the `splits` variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Vectorstores\n",
    "\n",
    "To build our database, we need an array of [Documents].\n",
    "\n",
    "With Chroma, this will be done locally. Note that there is no directory referencing our Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulgn0EGVTR71",
    "outputId": "5085b155-10d9-40fc-e580-1ba2f60ae77e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'sample_data']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8OlQJTcTR71"
   },
   "source": [
    "Next, we'll create the vectorstore considering the document split, embedding method, and the location of the vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUoXmsk3TR71",
    "outputId": "02ef501f-5540-4ac7-e4d6-81cabb3503ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = './vector_db_chroma/'\n",
    "\n",
    "!rm -rf ./docs/chroma  # remove old database files if any\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNIo00Y7Uheb",
    "outputId": "9ed32c86-1383-4a4c-9f28-697d793f57a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'vector_db_chroma', 'sample_data']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGdTzhWiTR71"
   },
   "source": [
    "Searches conducted in the Chroma database will yield pieces of information that match the \"intention\" of the question, which will subsequently serve for response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO3QHX0cTR71"
   },
   "outputs": [],
   "source": [
    "query_1 = vectordb.similarity_search(\n",
    "    \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\",\n",
    "    k=3,\n",
    ")\n",
    "query_2 = vectordb.similarity_search(\n",
    "    \"How does the LayoutParser library address the challenges mentioned in the summary and contribute to streamlining the usage of deep learning in DIA research and applications?\",\n",
    "    k=3,\n",
    ")\n",
    "# print(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "1Q96icY9TR71",
    "outputId": "476fda09-f187-4fdc-f6bf-225430d92e3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'image processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces6; y'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbeRs1fJTR71"
   },
   "source": [
    "## Retrieval\n",
    "\n",
    "A vector store retriever utilizes a vector store for document retrieval. It acts as a simplified interface to the vector store class, enabling compatibility with the retriever interface. This retriever leverages search functionalities provided by the vector store, such as similarity search and MMR, to retrieve texts stored within it.\n",
    "\n",
    "By default, the LangChain retriever object uses semantic similarity, which takes $k$ (4 by default) documents whose embeddings are closest to the query vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2cR0NFZTR71",
    "outputId": "688e0609-666c-44d1-951f-fea408fde6d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2 Z. Shen et al.\\n37], layout detection [ 38,22], table detection [ 26], and scene text detection [ 4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [ 1] or PyTorch [ 24], and the high-level parameters can\\nbe obfuscated by implementation details [ 8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,', metadata={'page': 1, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='image processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [ 21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [ 15] and the DeepDIVA project [ 2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14],easyOCR11and\\npaddleOCR12usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [ 35],\\n6The number shown is obtained by specifying the search type as ‘code’.\\n7https://ocr-d.de/en/about\\n8https://github.com/BobLd/DocumentLayoutAnalysis\\n9https://github.com/leonlulu/DeepLayout', metadata={'page': 2, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and', metadata={'page': 0, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='andTensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [ 3](magazine layouts),\\nPubLayNet [ 38](academic paper layouts), Table Bank [ 18](tables in academic\\npapers), Newspaper Navigator Dataset [ 16,17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3 The Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered', metadata={'page': 3, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\"\n",
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cme4ao6JTR71"
   },
   "source": [
    "### Maximum Marginal Relevance Retrieval (MMR)\n",
    "\n",
    "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance (`mmr`) search, you can specify that as the search type. Maximum Marginal Relevance is a process that takes into account the diversity of the semantic content of the documents to be outputed. The way this works:\n",
    "\n",
    "1. First, it conducts a semantic search and finds a larger initial pool of documents (20 by default)\n",
    "2. From those documents, it takes the most relevant one (the one with the smallest distance from the query).\n",
    "3. Adds this document to the pool of final relevant picks. We call this set $R$\n",
    "4. For the documents not in $R$, generate a score that rewards relevance and penalizes the proximity to the closest document in $R$.\n",
    "5. Add the document with the highest score to $R$\n",
    "6. Repeat 4 and 5 until $R$ has as many documents as we want (4 by default)\n",
    "\n",
    "For more details, see [Carbonell and Goldstein (1998)](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RudDc_myTR71",
    "outputId": "250e6b8e-b266-41f2-b03f-4aea6878f63d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2 Z. Shen et al.\\n37], layout detection [ 38,22], table detection [ 26], and scene text detection [ 4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [ 1] or PyTorch [ 24], and the high-level parameters can\\nbe obfuscated by implementation details [ 8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,', metadata={'page': 1, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='image processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [ 21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [ 15] and the DeepDIVA project [ 2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14],easyOCR11and\\npaddleOCR12usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [ 35],\\n6The number shown is obtained by specifying the search type as ‘code’.\\n7https://ocr-d.de/en/about\\n8https://github.com/BobLd/DocumentLayoutAnalysis\\n9https://github.com/leonlulu/DeepLayout', metadata={'page': 2, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\\nVi´ egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\\nhttps://www.tensorflow.org/ , software available from tensorﬂow.org\\n[2]Alberti, M., Pondenkandath, V., W¨ ursch, M., Ingold, R., Liwicki, M.: Deepdiva: a\\nhighly-functional python framework for reproducible experiments. In: 2018 16th\\nInternational Conference on Frontiers in Handwriting Recognition (ICFHR). pp.\\n423–428. IEEE (2018)\\n[3]Antonacopoulos, A., Bridson, D., Papadopoulos, C., Pletschacher, S.: A realistic\\ndataset for performance evaluation of document layout analysis. In: 2009 10th\\nInternational Conference on Document Analysis and Recognition. pp. 296–300.\\nIEEE (2009)\\n[4]Baek, Y., Lee, B., Han, D., Yun, S., Lee, H.: Character region awareness for text\\ndetection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. pp. 9365–9374 (2019)\\n[5]Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale\\nHierarchical Image Database. In: CVPR09 (2009)\\n[6]Deng, Y., Kanervisto, A., Ling, J., Rush, A.M.: Image-to-markup generation with\\ncoarse-to-ﬁne attention. In: International Conference on Machine Learning. pp.\\n980–989. PMLR (2017)\\n[7]Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.', metadata={'page': 13, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'}),\n",
       " Document(page_content='pp. 770–778 (2016)\\n[14] Kay, A.: Tesseract: An open-source optical character recognition engine. Linux J.\\n2007 (159), 2 (Jul 2007)\\n[15] Lamiroy, B., Lopresti, D.: An open architecture for end-to-end document analysis\\nbenchmarking. In: 2011 International Conference on Document Analysis and\\nRecognition. pp. 42–47. IEEE (2011)\\n[16]Lee, B.C., Weld, D.S.: Newspaper navigator: Open faceted search for 1.5\\nmillion images. In: Adjunct Publication of the 33rd Annual ACM Sym-\\nposium on User Interface Software and Technology. p. 120–122. UIST\\n’20 Adjunct, Association for Computing Machinery, New York, NY, USA\\n(2020). https://doi.org/10.1145/3379350.3416143, https://doi-org.offcampus.\\nlib.washington.edu/10.1145/3379350.3416143\\n[17]Lee, B.C.G., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N.,\\nThomas, D., Zwaard, K., Weld, D.S.: The Newspaper Navigator Dataset: Extracting\\nHeadlines and Visual Content from 16 Million Historic Newspaper Pages in\\nChronicling America, p. 3055–3062. Association for Computing Machinery, New\\nYork, NY, USA (2020), https://doi.org/10.1145/3340531.3412767\\n[18] Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: Table benchmark\\nfor image-based table detection and recognition. arXiv preprint arXiv:1903.01949\\n(2019)\\n[19] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,\\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\\non computer vision. pp. 740–755. Springer (2014)', metadata={'page': 14, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiytPD2tTR71"
   },
   "source": [
    "### Similarity Score Threshold Retrieval\n",
    "\n",
    "Sets a similarity score threshold and only returns documents with a score above that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojF_8tLUTR72",
    "outputId": "bb41ff8b-4007-40b7-c250-96383c0fdf61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='image processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [ 21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [ 15] and the DeepDIVA project [ 2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14],easyOCR11and\\npaddleOCR12usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [ 35],\\n6The number shown is obtained by specifying the search type as ‘code’.\\n7https://ocr-d.de/en/about\\n8https://github.com/BobLd/DocumentLayoutAnalysis\\n9https://github.com/leonlulu/DeepLayout', metadata={'page': 2, 'source': 'https://arxiv.org/pdf/2103.15348.pdf'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    ")\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRGd-PD-TR72"
   },
   "source": [
    "<!-- retrieval -->\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "Let's review the model setup. We have our vectorstore, we ask a question, and the vectorstore returns relevant elements to answer the question. Since these are parts of the document, they need to be passed through an LLM engine to structure (`chain`) a coherent response. Typically, within these models, there exists a parameter called `temperature` where 0 is the most precise and 1 makes the model \"ultra creative.\" This final step can be done in various ways.\n",
    "\n",
    "- `stuff`: Prepares and organizes input data or parameters.\n",
    "- `map_reduce`: Distributes computation tasks across multiple nodes or processes, often used for parallel processing and aggregating results.\n",
    "- `refine`: Improves the quality or accuracy of output by iteratively adjusting parameters or fine-tuning the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGOvXIL7TR72",
    "outputId": "e7e8d822-0d3e-4ca0-a12f-ef3ee9dd7756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA as RQa\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)\n",
    "question = \"What are some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZJcxwo-TR72"
   },
   "source": [
    "### Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "CtWi7mHeTR72",
    "outputId": "78634568-a024-4301-8f5e-1b1083089ad7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA) include:\\n\\n1. Complexity of Deep Learning Models: Deep learning models used in DIA are often convoluted and developed using different frameworks like TensorFlow or PyTorch. This complexity makes it challenging to reuse and extend existing models, as high-level parameters can be obfuscated by implementation details.\\n\\n2. Lack of Infrastructure for Customized Training: Document images contain diverse patterns across domains, requiring customized training for desirable detection accuracy. Currently, there is no full-fledged infrastructure for easily curating target document image datasets and fine-tuning or re-training models.\\n\\n3. Sequential Processing Requirements: DIA often requires a sequence of models and processing steps to obtain final outputs. This sequential processing can complicate the adoption of deep learning models, as research teams may need to use multiple models and perform further analyses in separate processes.\\n\\n4. Limited Functionalities in Existing Codebases: Existing codebases for DIA often rely on traditional rule-based methods or provide limited functionalities for deep learning-based approaches. This limitation hinders the adoption of recent advances in deep learning for DIA tasks.\\n\\nThese challenges make it difficult for researchers and practitioners to easily deploy, extend, and reuse innovations in DIA, unlike disciplines like natural language processing and computer vision where there are more streamlined tools and frameworks available.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff = RQa.from_chain_type(\n",
    "    llm, retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"stuff\" # default\n",
    ")\n",
    "stuff_result = stuff({\"query\": question})\n",
    "stuff_result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGKcaykxTR72"
   },
   "source": [
    "### Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "tsRDmgNcTR72",
    "outputId": "4080a117-d6ec-4137-dc62-41fc0b0c2452"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Some of the challenges hindering the widespread adoption and reuse of innovations in document image analysis (DIA), particularly in comparison to disciplines like natural language processing and computer vision, include factors like the convoluted nature of deep learning models for reuse and extension, the lack of a full-fledged infrastructure for easily curating target document image datasets, the reliance on traditional rule-based methods in existing code pieces, limited functionalities in available tools, lack of standardized datasets and benchmarks for DIA tasks, and the lack of active maintenance in platforms supporting DIA.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_p = RQa.from_chain_type(\n",
    "    llm, retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "mp_result = m_p({\"query\": question})\n",
    "mp_result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2ggDaQGTR72"
   },
   "source": [
    "### Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "jB540y-6TR72",
    "outputId": "339b61d9-d072-4ead-a051-0fac326aecb0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The additional context provided highlights the advancements in document image analysis (DIA) tools and datasets, such as TensorFlow Hub and various document data collections, that aim to facilitate the development and sharing of pretrained models and pipelines specific to DIA tasks. These resources, along with the LayoutParser model zoo, offer a spectrum of models trained on diverse datasets to support different use cases in document analysis.\\n\\nIn light of this context, the challenges hindering the widespread adoption and reuse of innovations in DIA, particularly in comparison to disciplines like natural language processing and computer vision, can be further refined:\\n\\n1. Fragmentation in DIA Tools and Models: Despite the availability of pretrained models and document data collections in DIA, the lack of standardized tools and pipelines for document analysis hinders the seamless integration and reuse of innovations across different stages of DIA tasks. This fragmentation in tools and models complicates the adoption of advanced techniques in document image analysis.\\n\\n2. Limited Optimization for DIA Challenges: While there have been efforts to develop pretrained models and pipelines specific to DIA tasks, the lack of optimization for the unique challenges faced in document analysis limits the widespread adoption of innovations in the field. Unlike disciplines like natural language processing and computer vision, where there are more comprehensive tools and frameworks, the optimization for DIA tasks remains a challenge.\\n\\n3. Interdisciplinary Research Support: The availability of diverse datasets and pretrained models in DIA, such as those in the LayoutParser model zoo, presents opportunities for interdisciplinary research. However, the lack of comprehensive tools and platforms tailored to the specific needs of interdisciplinary research in document image analysis hinders the widespread adoption and reuse of innovations in the field.\\n\\nBy addressing these refined challenges, such as the fragmentation in DIA tools and models, the limited optimization for DIA challenges, and the need for better support for interdisciplinary research, tools like LayoutParser can play a crucial role in streamlining the adoption and reuse of innovations in document image analysis.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine = RQa.from_chain_type(\n",
    "    llm, retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "refine_result = refine({\"query\": question})\n",
    "refine_result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBWcXempTR72"
   },
   "source": [
    "### Question Answering With Prompt\n",
    "\n",
    "Now, a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start#prompttemplate) will be created to guide us in answering the question, instructing the model on how to use the provided context to generate concise answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PWdHfbBTR73"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "Dzb1JbyMTR73",
    "outputId": "7a185367-8b04-4db6-876c-e22a0df0ad66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Some challenges hindering the widespread adoption and reuse of innovations in DIA include the convoluted nature of deep learning models, the lack of infrastructure for curating datasets and re-training models, and the need for a sequence of models and processing steps for final outputs. These challenges make it difficult for researchers without technical backgrounds to implement and adapt existing models for DIA. Thanks for asking!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run chain\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "promt_result = qa_chain({\"query\": question})\n",
    "promt_result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK9nhFbiTR73"
   },
   "source": [
    "## RAG\n",
    "\n",
    "After completing the previous step, we have all the relevant information for the natural language model to interpret and provide a response considering the context and the question.\n",
    "\n",
    "To summarize, first, we need to set up the environment by providing the API key for OpenAI and adding it to our virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7Y_ogDOTR73",
    "outputId": "de986ed9-caf6-4721-d1fc-e3760799fa91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI - API-KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "import getpass, openai, os\n",
    "import getpass, openai, os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA as RQa\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"OPENAI - API-KEY: \")\n",
    "openai.apikey = api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phsqJDbdTR73"
   },
   "source": [
    "Next, we load the document, in this case, we'll use a thesis from PUCP (https://tesis.pucp.edu.pe/repositorio/handle/20.500.12404/27052)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "Cwf5GtDVTR73",
    "outputId": "c3624d4b-0c77-4d61-9073-5497d44e05fd"
   },
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='tesis.pucp.edu.pe', port=443): Max retries exceeded with url: /repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%C3%8D_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     ssl_sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mssl_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssl_wrap_socket_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_sock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='tesis.pucp.edu.pe', port=443): Max retries exceeded with url: /repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%C3%8D_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2633e605b14f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tesis.pucp.edu.pe/repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%c3%8d_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;34m\"pypdf package not found, please install it with \"\u001b[0m \u001b[0;34m\"`pip install pypdf`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_s3_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='tesis.pucp.edu.pe', port=443): Max retries exceeded with url: /repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%C3%8D_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "url_pdf = \"https://tesis.pucp.edu.pe/repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%c3%8d_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y\"\n",
    "loader = PyPDFLoader(url_pdf)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWf07jR_TR73"
   },
   "source": [
    "Then, we define the splitting module to generate text chunks with the previously loaded `Document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iZIFfM9TR73",
    "outputId": "ccf393a4-02b3-4458-bd08-5abbb9ed2483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir3S8IJWTR73"
   },
   "source": [
    "Finally, we generate the vector database (thesis) using the OPENAI embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItrFsaEzTR73"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "persist_directory = './thesis_chroma/'\n",
    "\n",
    "# !rm -rf ./thesis_chroma  # remove old database files if any (linux, Mac)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi9ZTf8NTR73"
   },
   "source": [
    "If you want to retrieve the previously created vectorbase, you only need to locate the directory of the database and pass the embedding method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8_Sk1J-TR74"
   },
   "outputs": [],
   "source": [
    " vectordb = Chroma(persist_directory=persist_directory, embedding_function = embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ0-niUmTR74"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA as RQa\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_model, temperature = 0)\n",
    "question = input(\"Ingrese una pregunta acerca de la tesis: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UJqVmbPTR74"
   },
   "outputs": [],
   "source": [
    "stuff = RQa.from_chain_type(\n",
    "    llm, retriever = vectordb.as_retriever(),\n",
    "    chain_type = \"stuff\" # default\n",
    ")\n",
    "result = stuff({\"query\": question})\n",
    "response = result['result']\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8li0YpoTR74"
   },
   "source": [
    "## Final Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsrJIXF-XMJa"
   },
   "source": [
    "### Pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXTuhZybTR74"
   },
   "outputs": [],
   "source": [
    "import getpass, openai, os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA as RQa\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Insert your OPENAI - API-KEY: \")\n",
    "openai.apikey = api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "\n",
    "url_pdf = input(\"Insert the pdfurl: \")\n",
    "\n",
    "loader = PyPDFLoader(url_pdf)\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(pages)\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# persist_directory = './thesis_chroma/'\n",
    "\n",
    "# !rm -rf ./thesis_chroma  # remove old database files if any (linux, Mac)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    # persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_model, temperature = 0)\n",
    "\n",
    "# example url: https://tesis.pucp.edu.pe/repositorio/bitstream/handle/20.500.12404/27040/HUAMAN%c3%8d_LLAMOCCA_ROGER_ANGEL_DESARROLLO_COMPETENCIAS.pdf?sequence=1&isAllowed=y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW-0Fq5UXY_M"
   },
   "source": [
    "### Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxApHpM5XK4y"
   },
   "outputs": [],
   "source": [
    "# ¿Qué mecanismos y/o procesos implementa el programa Mibeca para la inserción laboral de los egresados?\n",
    "# ¿Cómo se evalúa la calidad de los servicios prestados por los IEST elegibles por el programa Mibeca para el desarrollo de competencias en los becarios?\n",
    "while True:\n",
    "    question = input(\"Ask: \")\n",
    "    if question == \"\":\n",
    "        break\n",
    "    stuff = RQa.from_chain_type(\n",
    "        llm, retriever = vectordb.as_retriever(),\n",
    "        chain_type = \"stuff\" # default\n",
    "    )\n",
    "    stuff_result = stuff({\"query\": question})\n",
    "    result = stuff_result['result']\n",
    "    format_response = f\"\"\"\n",
    "    Question:\n",
    "      {question}\n",
    "    Result:\n",
    "      {result}\n",
    "    ------------ x -------------\n",
    "    \"\"\"\n",
    "    print(format_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ad6m_YXvj3"
   },
   "source": [
    "### Several documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN0j3GdqXxhB"
   },
   "outputs": [],
   "source": [
    "example_urls = [\n",
    "    \"https://www.defensoria.gob.pe/wp-content/uploads/2023/02/Reporte-Mensual-de-Conflictos-Sociales-N%C2%B0-227-Enero-2023.pdf\",\n",
    "    \"https://www.defensoria.gob.pe/wp-content/uploads/2023/03/Reporte-Mensual-de-Conflictos-Sociales-N%C2%B0-228-Febrero-2023.pdf\",\n",
    "    \"https://www.defensoria.gob.pe/wp-content/uploads/2023/04/Reporte-Mensual-de-Conflictos-Sociales-N-229-Marzo-2023.pdf\"\n",
    "]\n",
    "\n",
    "!rm -rf ./vector_db_chroma/  # remove old database files if any (linux, Mac)\n",
    "\n",
    "for i, url_pdf in enumerate(example_urls):\n",
    "\n",
    "    loader = PyPDFLoader(url_pdf)\n",
    "    pages = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1500,\n",
    "        chunk_overlap = 150\n",
    "    )\n",
    "    splits = text_splitter.split_documents(pages)\n",
    "    embedding = OpenAIEmbeddings()\n",
    "\n",
    "    # persist_directory = './thesis_chroma/'\n",
    "\n",
    "    # !rm -rf ./thesis_chroma  # remove old database files if any (linux, Mac)\n",
    "    if i == 0:\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding\n",
    "        )\n",
    "    else:\n",
    "        vectordb.add_documents(\n",
    "            documents=splits\n",
    "        )\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_model, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34BNlAr5aStm"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Ask: \")\n",
    "    if question == \"\":\n",
    "        break\n",
    "    stuff = RQa.from_chain_type(\n",
    "        llm, retriever = vectordb.as_retriever(),\n",
    "        chain_type = \"stuff\" # default\n",
    "    )\n",
    "    stuff_result = stuff({\"query\": question})\n",
    "    result = stuff_result['result']\n",
    "    format_response = f\"\"\"\n",
    "    Question:\n",
    "      {question}\n",
    "    Result:\n",
    "      {result}\n",
    "    ------------ x -------------\n",
    "    \"\"\"\n",
    "    print(format_response)\n",
    "\n",
    "# example questions:\n",
    "# cómo afectan los choques de política fiscal no anticipados a la economía?\n",
    "# la política monetaria afecta de manera distinta a sectores distintos?\n",
    "# cómo afectan los acuerdos comerciales al valor de las exportaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fHgQKGUd5xK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
